{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradABM Demo\n",
    "This notebook is an example of training Two-in-on architecture on the bogota dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch/bxv6gs/GradABM/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from epiweeks import Week\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import traceback\n",
    "from copy import copy\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdb \n",
    "import json\n",
    "from src.utils import save_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta, Disease: bogota, Seed: 2345, Config file: Configs/bogota.json\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Updated arguments using argparse.Namespace\n",
    "args = argparse.Namespace(\n",
    "    model_name='meta',                          # Updated from 'GradABM' to 'meta'\n",
    "    disease='bogota',                           # Updated from 'COVID' to 'bogota'\n",
    "    seed=2345,                                  # Updated from 6666 to 2345\n",
    "    num_runs=1,                                 # Default remains unchanged\n",
    "    state='MA',                                 # Updated to 'MA'\n",
    "    county_id='25001',                          # Default remains unchanged\n",
    "    dev=['0'],                                  # Updated to ['0'] based on script\n",
    "    pred_ew='202021',                           # Default remains unchanged\n",
    "    joint=True,                                 # Updated to True\n",
    "    inference_only=False,                       # Default remains unchanged\n",
    "    noise=0,                                    # Default remains unchanged\n",
    "    results_file_postfix='',                    # Default remains unchanged\n",
    "    dataset=\"./Data/Processed/county_data.csv\", # Default remains unchanged\n",
    "    date=\"0_moving\",                           # Updated from '03-03' to reflect script variable\n",
    "    note=\"0_moving\",                           # Updated to reflect script variable\n",
    "    privacy=False,                              # Default remains unchanged\n",
    "    Delta=1,                                    # Default remains unchanged\n",
    "    x=5,                                        # Default remains unchanged\n",
    "    counterfactual=False,                       # Default remains unchanged\n",
    "    week=49,                                     # Updated to match $target_week\n",
    "    config_file=\"Configs/bogota.json\"           # Updated to 'Configs/bogota.json'\n",
    ")\n",
    "\n",
    "# Access arguments as args.xxx\n",
    "print(f\"Model: {args.model_name}, Disease: {args.disease}, Seed: {args.seed}, Config file: {args.config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed used for python random, numpy and torch is 2345\n",
      "devices used: [device(type='cuda', index=0)]\n",
      "---- MAIN IMPORTS SUCCESSFUL -----\n",
      "Run:  0\n",
      "Scaler Prepared\n",
      "epoch  49\n",
      "epoch_loss 7.488403797149658 0\n",
      "epoch  99\n",
      "epoch_loss 4.898044109344482 0\n",
      "epoch  149\n",
      "epoch_loss 3.7387123107910156 0\n",
      "epoch  199\n",
      "epoch_loss 2.983400344848633 0\n",
      "epoch  249\n",
      "epoch_loss 2.5649795532226562 0\n",
      "epoch  299\n",
      "epoch_loss 2.300121784210205 0\n",
      "epoch  349\n",
      "epoch_loss 2.070878267288208 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(args\u001b[38;5;241m.\u001b[39mconfig_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     configs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m---> 14\u001b[0m counties_predicted, predictions, learned_params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m num_counties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(counties_predicted)\n\u001b[1;32m     16\u001b[0m save_params(disease,model_name,pred_ew,learned_params, args)\n",
      "File \u001b[0;32m/sfs/weka/scratch/bxv6gs/GradABM/src/train_abm_two_in_one.py:636\u001b[0m, in \u001b[0;36mtrain_predict\u001b[0;34m(args, configs)\u001b[0m\n\u001b[1;32m    633\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# start training!\u001b[39;00m\n\u001b[0;32m--> 636\u001b[0m counties_predicted, predictions, learned_params \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m counties_predicted, predictions, learned_params\n",
      "File \u001b[0;32m/sfs/weka/scratch/bxv6gs/GradABM/src/train_abm_two_in_one.py:370\u001b[0m, in \u001b[0;36mrunner\u001b[0;34m(params, devices, verbose, args)\u001b[0m\n\u001b[1;32m    367\u001b[0m loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\u001b[38;5;241m*\u001b[39m(loss_weight\u001b[38;5;241m*\u001b[39mloss_fcn(y, predictions))\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m scale \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m loss_fcn(lstm_targets[:, :, \u001b[38;5;241m0\u001b[39m], predictions_2_in_1)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# print((loss_weight*loss_fcn(y, predictions)).mean().sqrt(), loss_fcn(lstm_targets, predictions_2_in_1).mean())\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(param_model\u001b[38;5;241m.\u001b[39mparameters(), CLIP)\n\u001b[1;32m    373\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/pets/lib/python3.9/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pets/lib/python3.9/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if args.counterfactual is False:\n",
    "    from src.train_abm_two_in_one import train_predict\n",
    "else:\n",
    "    from src.train import train_predict\n",
    "\n",
    "disease = args.disease\n",
    "model_name = args.model_name\n",
    "pred_ew = Week.fromstring(args.pred_ew)\n",
    "args.pred_week = pred_ew.cdcformat()\n",
    "\n",
    "with open(args.config_file, \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "counties_predicted, predictions, learned_params = train_predict(args, configs)\n",
    "num_counties = len(counties_predicted)\n",
    "save_params(disease,model_name,pred_ew,learned_params, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pets",
   "language": "python",
   "name": "pets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
